- 爬虫和反爬虫之间的斗争,最后一定是爬虫获胜!
Why? 只要是真实用户可以浏览的网页数据,爬虫就一定能爬下来
======================================================================================
- 根据使用场景通用爬虫和聚焦爬虫
1.通用爬虫:搜索引擎用的爬虫系统
    1.目标:就是尽可能把互联网上所有的网页下载下来,放到
          本地服务器里形成备份,再对这些网页做相关处理.
    2.抓取流程:
        a)首选选取一部分已有的URL,把这些URL放到带爬取队列
        b)从队列里取出这些URL,然后解析DNS得到主机IP,然后去着个Ip对应
          的服务器里下载HTML页面,保存到搜索引擎的服务器里,之后把这个爬
          郭的URL放入到已爬取队列.
        c)分析这些网页网页内用,找出网页里其他的URL连接,继续执行第二部,
          直到爬取条件结束
    3.搜索引擎是如何获取一个新网站的URL:
        1.主动向搜索引擎提交网址/    http://zhangzhang.baidu.com/linksubmit/url  百度站长
        2.在其他网站设置网站外链接
        3.搜索引擎会和DNS服务商进行合作,可以快速收录新的网站
    4.通用爬虫并不是万物皆可爬,它也需要遵守规则:
        Robots协议:协议会指明通用爬虫可以爬取网页的权限
        Robots协议并不是所有爬虫都蹲守的,一般只有大型的搜索引擎爬虫才会遵守
        咱们个人学的爬虫没关系
    5.通用爬虫的工作流程:爬取网页-->存储数据-->内容处理-->提供检索/服务排名

    6.搜索引擎排名:
        1.pageRank值:用户浏览量(点击量/浏览量/人气)统计,流量越高,排名也越值钱
        2.竞价排名:谁过钱多,谁排名越高. 
    7.通用爬虫的缺点:
        1.只能提供和文本相关的内容(HTML,Word,PDF)等等,但是不能提供多媒体(音乐,图片,视频)和二进制文件(程序,脚本)等
        2.提供的结果前篇一律,不能针对不同背景领域的人提供不同的搜索结果
        3.不能人类语义上的检索
     
    DNS:就是把域名解析成IP的一种技术

    为了解决通用爬虫不能做的事情,由此出现了聚焦爬虫
    聚焦爬虫:爬虫程序员写的针对某种内容的爬虫
    面向主题爬虫,面向需求爬虫,会针对某种特定的内容去爬取信息,而且会保证信息和需求尽可能相关

2.HTTP和HTTPS
HTTP协议(HyperText Transfer Protocol,超文本传输协议):是一种发布和接收HTML页面的方法
HTTPS(HyperText Transfer Protocol over Secure Socket Layer)简单讲是HTTP的安全版,在HTTP下加入SSL层
- SSL(Secure Socket Layer安全套阶层)主要用于Web的安全传输协议,在传输层对网络连接进行加密,保障在Internet上
    数据传输的安全
    - HTTP的端口号是80
    - HTTPS的端口号是443

HTTP工作原理
网络爬虫抓取过程可以理解为 模拟浏览器的操作的过程
浏览器的主要功能是向服务器发出请求,在浏览器窗口中展示您选择的网络资源,HTTP是一套计算机通过网络进行通信的规则

http的请求类别:
    Get:使用get请求时,所有的查询参数都会在url上面显示出来
    Post:请求时提交的数据都是隐藏在form表单里面的

Cookie 和 Session
服务器和客户端的交互仅限于请求/响应过程,结束之后便断开,在下一次请求时,服务器会认为
新的客户端.
为了维护他们之间的链接.让服务器知道这是前一个用户发送的请求,必须在一个地方保存客户端信息.

Cookie:通过在客户端记录的信息确定用户的身份
Session:通过在服务器端记录的信息确定用户的身份
 